{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "import warnings\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import os\n",
    "import bottleneck as bn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import signal\n",
    "from scipy.special import gamma, psi\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.feature_selection._base import SelectorMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB\n",
    "from sklearn.utils import check_X_y\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from src.train import cv, calculate_score, prepare_cv_indices\n",
    "from src.feature_selector import BaseFeatureSelector\n",
    "from src.custom_feature_selectors.mi import MIFeatureSelector, CMIM, JMIM, IGFS\n",
    "from src.custom_feature_selectors.boruta import Boruta\n",
    "from src.custom_feature_selectors.feature_importance import RandomForestFeatureImportanceSelector\n",
    "from src.settings import DATA_DIR\n",
    "from src.experiment_utils import perform_experiments, find_best_experiments\n",
    "from src.experiment import Experiment\n",
    "\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 500) (5000,)\n",
      "(5000, 500)\n"
     ]
    }
   ],
   "source": [
    "# Read the text file into a dataframe\n",
    "X = pd.read_csv(os.path.join(DATA_DIR, 'x_train.txt'), sep=' ', header=None).to_numpy()\n",
    "y = pd.read_csv(os.path.join(DATA_DIR, 'y_train.txt'), header=None).to_numpy().T[0]\n",
    "X_test = pd.read_csv(os.path.join(DATA_DIR, 'x_test.txt'), sep=' ', header=None).to_numpy()\n",
    "\n",
    "# Read the shape of the data\n",
    "print(X.shape, y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.copy()\n",
    "X_train_orig = X.copy()\n",
    "\n",
    "transformer = FunctionTransformer(np.log1p)\n",
    "X_train = np.concatenate(\n",
    "    [X_train, transformer.fit_transform(X_train_orig)], axis=1\n",
    ")\n",
    "\n",
    "transformer2 = FunctionTransformer(np.sqrt)\n",
    "X_train = np.concatenate(\n",
    "    [X_train, transformer2.fit_transform(X_train_orig)], axis=1\n",
    ")\n",
    "\n",
    "transformer3 = FunctionTransformer(np.exp)\n",
    "X_train = np.concatenate(\n",
    "    [X_train, transformer3.fit_transform(X_train_orig)], axis=1\n",
    ")\n",
    "\n",
    "if np.isnan(X_train).any():\n",
    "    # if so, replace NaNs with 0\n",
    "    X_train = np.nan_to_num(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import tree\n",
    "\n",
    "\n",
    "# class AdaBoost():\n",
    "    \n",
    "#     def __init__(self, max_depth=1):\n",
    "#         self.max_depth = max_depth\n",
    "#         self.models = []\n",
    "#         self.betas = []\n",
    "#         self.weights = None\n",
    "#         self.n_classes = None\n",
    "#         self._stopped = False\n",
    "    \n",
    "#     def fit(self, X, y, n_iter=100):\n",
    "#         n = X.shape[0]\n",
    "#         if self.weights is None:\n",
    "#             self.weights = np.ones(n) / n\n",
    "#             self.n_classes = len(np.unique(y))\n",
    "        \n",
    "#         for i in range(n_iter):\n",
    "#             model = tree.DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "#             model.fit(X, y, sample_weight = self.weights)\n",
    "#             y_pred = model.predict(X)\n",
    "#             err = self.calculate_error(y, y_pred)\n",
    "#             if err < 1e-14:\n",
    "#                 self._stopped = True\n",
    "#                 break\n",
    "\n",
    "#             self.models.append(model)\n",
    "#             self.betas.append(err / (1 - err))\n",
    "#             self.update_weights(y, y_pred)\n",
    "            \n",
    "#     def predict(self, X):\n",
    "#         n_models = len(self.models)\n",
    "#         class_preds = np.zeros((X.shape[0], self.n_classes))\n",
    "        \n",
    "#         for i in range(n_models):\n",
    "#             y_pred = self.models[i].predict(X)\n",
    "#             for j in range(self.n_classes):\n",
    "#                 y_weighted_pred = (y_pred == np.ones(y_pred.shape[0]) * j) * np.log(1 / self.betas[i])\n",
    "#                 class_preds[:, j] = class_preds[:, j] + y_weighted_pred\n",
    "                \n",
    "#         return np.argmax(class_preds, axis=1)\n",
    "        \n",
    "#     def calculate_error(self, y_true, y_pred):\n",
    "#         return np.sum((y_true != y_pred) * self.weights)\n",
    "    \n",
    "#     def update_weights(self, y_true, y_pred):\n",
    "#         scaler = np.ones(self.weights.shape[0])\n",
    "#         scaler[y_true == y_pred] = self.betas[-1]\n",
    "#         self.weights = self.weights * scaler\n",
    "#         sum_weights = np.sum(self.weights)\n",
    "#         self.weights /= sum_weights\n",
    "\n",
    "#     def predict_proba(self, X):\n",
    "#         n_models = len(self.models)\n",
    "#         class_preds = np.zeros((X.shape[0], self.n_classes))\n",
    "        \n",
    "#         for i in range(n_models):\n",
    "#             y_pred = self.models[i].predict(X)\n",
    "#             for j in range(self.n_classes):\n",
    "#                 y_weighted_pred = (y_pred == np.ones(y_pred.shape[0]) * j) * np.log(1 / self.betas[i])\n",
    "#                 class_preds[:, j] = class_preds[:, j] + y_weighted_pred\n",
    "                \n",
    "#         return class_preds / np.sum(class_preds, axis=1).reshape(-1, 1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "n_features = 3\n",
    "\n",
    "experiment_config = [\n",
    "    \n",
    "    # Experiment(\n",
    "    #     classifier=GradientBoostingClassifier,\n",
    "    #     classifier_config = {\n",
    "    #         \"n_estimators\":600, \n",
    "    #         \"learning_rate\":1.2,\n",
    "    #         \"max_depth\":1,\n",
    "    #         \"n_iter_no_change\": 10,\n",
    "    #         \"min_samples_split\": 20,\n",
    "\n",
    "    #     },\n",
    "    #     feature_selector=RandomForestFeatureImportanceSelector,\n",
    "    #     feature_selector_config={\n",
    "    #         \"n_features\": n_features,\n",
    "    #     }\n",
    "    # ),\n",
    "\n",
    "    # Experiment(\n",
    "    #     classifier=AdaBoostClassifier,\n",
    "    #     classifier_config = {\n",
    "    #         \"n_estimators\": 300,\n",
    "    #         \"learning_rate\": 0.1,\n",
    "    #     },\n",
    "    #     feature_selector=RandomForestFeatureImportanceSelector,\n",
    "    #     feature_selector_config={\n",
    "    #         \"n_features\": n_features,\n",
    "    #     }\n",
    "    # ),\n",
    "\n",
    "    # Experiment(\n",
    "    #     classifier=MLPClassifier,\n",
    "    #     classifier_config = {'activation': 'relu', \n",
    "    #                          'alpha': 1e-05, \n",
    "    #                          'hidden_layer_sizes': (100, 100, 50,), \n",
    "    #                          'learning_rate': 'constant', \n",
    "    #                          'learning_rate_init': 0.001, \n",
    "    #                          'solver': 'sgd'\n",
    "    #                          },\n",
    "    #     feature_selector=RandomForestFeatureImportanceSelector,\n",
    "    #     feature_selector_config={\n",
    "    #         \"n_features\": n_features,\n",
    "    #     }\n",
    "    # ),\n",
    "\n",
    "    # Experiment(\n",
    "    #     classifier=StackingClassifier,\n",
    "    #     classifier_config = {'estimators': [\n",
    "    #         ('gnb', GaussianNB()),\n",
    "    #         ('qda', QDA()),\n",
    "    #         ('ada', AdaBoostClassifier(n_estimators=100)),\n",
    "    #         ],\n",
    "    #         'final_estimator': LogisticRegression(penalty='l2', C=1, solver='lbfgs', max_iter=1000),\n",
    "    #     },\n",
    "    #     feature_selector=RandomForestFeatureImportanceSelector,\n",
    "    #     feature_selector_config={\n",
    "    #         \"n_features\": n_features,\n",
    "    #     },\n",
    "    #     # transformer=True,\n",
    "    # ),\n",
    "    Experiment(\n",
    "        classifier=VotingClassifier,\n",
    "        classifier_config = {'estimators': [\n",
    "            ('gnb', GaussianNB()),\n",
    "            ('qda', QDA()),\n",
    "            ('ada', AdaBoostClassifier(n_estimators=100)),\n",
    "            ],\n",
    "            'voting': 'soft',\n",
    "        },\n",
    "        feature_selector=RandomForestFeatureImportanceSelector,\n",
    "        feature_selector_config={\n",
    "            \"n_features\": n_features,\n",
    "        },\n",
    "        # transformer=True,\n",
    "    ),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment exp_vc_rffis_a20427 in progress...\n",
      "Using 3 features, we properly classified 150/200 clients.\n",
      "Using 3 features, we properly classified 151/200 clients.\n",
      "Using 3 features, we properly classified 151/200 clients.\n",
      "Using 3 features, we properly classified 152/200 clients.\n",
      "Using 3 features, we properly classified 155/200 clients.\n"
     ]
    }
   ],
   "source": [
    "scores, indices = perform_experiments(\n",
    "    X[:, [0, 1, 100, 102, 103]], \n",
    "    y, experiment_config)\n",
    "# X_train[:, [8, 100,  101,  102, 103, 104, 105, 605, 1100, 1508, 1600, 1601, 1602, 1603, 1604, 1605]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp_sc_jmim_f47b41': [array([0, 1, 2]),\n",
       "  array([0, 1, 2]),\n",
       "  array([0, 1, 2]),\n",
       "  array([0, 1, 2]),\n",
       "  array([0, 1, 2])]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "unique_indices = set()\n",
    "\n",
    "# Iterate over all arrays in the dictionary and add their elements to the set\n",
    "for key in indices:\n",
    "    for array in indices[key]:\n",
    "        unique_indices.update(array)\n",
    "\n",
    "# Convert the set back to a sorted list\n",
    "unique_indices = sorted(list(unique_indices))\n",
    "\n",
    "print(unique_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_best_indices = []\n",
    "# for id in [0, 1, 2, 3, 4, 7, 8, 9, 11, 15, 16, 17, 27, 28, 29]:\n",
    "#     small_best_indices.append(best_indices[id])\n",
    "# small_best_indices\n",
    "small_best_indices = [0, 1, 2, 3, 4, 7, 29, 34, 40, 100, 101, 102, 103, 105, 144, 340, 497]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 16, 17, 29, 34, 40, 45, 48, 74, 100, 101, 102, 103, 104, 105, 144, 340, 497]\n",
    "\n",
    "X[:, best_indices].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 500 features, we properly classified 176/333 clients.\n",
      "Using 500 features, we properly classified 187/333 clients.\n",
      "Using 500 features, we properly classified 165/333 clients.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-94721.05578884223, -94391.12177564488, -95048.01920768307]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# model = RandomForestClassifier(max_depth=5)\n",
    "model = QuadraticDiscriminantAnalysis()\n",
    "k_folds = 3\n",
    "\n",
    "scores = cv(X, y, model, k_folds)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 27/500 [01:14<21:38,  2.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m feature_selector \u001b[38;5;241m=\u001b[39m MIFS(n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m k_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 5\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_selector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_selector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m scores\n",
      "File \u001b[0;32m~/github/AML-2024L-Offer-Acceptance-Prediction/src/train.py:92\u001b[0m, in \u001b[0;36mcv\u001b[0;34m(X, y, model, k_folds, feature_selector)\u001b[0m\n\u001b[1;32m     89\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m X[train_indices], y[train_indices]\n\u001b[1;32m     90\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m X[test_indices], y[test_indices]\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_selector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     feature_selector\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     94\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m feature_selector\u001b[38;5;241m.\u001b[39mtransform(X_train)\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mMIFS.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_)):\n\u001b[0;32m---> 12\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mifs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores_[i] \u001b[38;5;241m=\u001b[39m s\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mranks_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores_)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[14], line 44\u001b[0m, in \u001b[0;36mMIFS._mifs\u001b[0;34m(self, X, y, i)\u001b[0m\n\u001b[1;32m     41\u001b[0m     d_i \u001b[38;5;241m=\u001b[39m d_ij[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     42\u001b[0m     d_j \u001b[38;5;241m=\u001b[39m d_ij[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m     mi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     scores[j] \u001b[38;5;241m=\u001b[39m mi\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(scores)\n",
      "Cell \u001b[0;32mIn[14], line 49\u001b[0m, in \u001b[0;36mMIFS._mi\u001b[0;34m(self, d_i, d_j)\u001b[0m\n\u001b[1;32m     45\u001b[0m         scores[j] \u001b[38;5;241m=\u001b[39m mi\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(scores)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mi\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_i, d_j):\n\u001b[1;32m     50\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(d_i)\n\u001b[1;32m     51\u001b[0m     n_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(n_samples))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_depth=5)\n",
    "feature_selector = MIFS(n_features_to_select=10)\n",
    "k_folds = 5\n",
    "\n",
    "scores = cv(X, y, model, k_folds, feature_selector=feature_selector)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MIFS(BaseFeatureSelector):\n",
    "\n",
    "    def __init__(self, n_features_to_select, n_neighbors=3):\n",
    "        super().__init__(n_features_to_select)\n",
    "        self.n_neighbors = n_neighbors\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.scores_ = np.zeros(self.n_features_)\n",
    "\n",
    "        for i in tqdm(range(self.n_features_)):\n",
    "            s = self._mifs(X, y, i)\n",
    "            self.scores_[i] = s\n",
    "\n",
    "        self.ranks_ = np.argsort(self.scores_)[::-1]\n",
    "        self.support_ = np.zeros(self.n_features_, dtype=bool)\n",
    "        self.support_[self.ranks_[: self.n_features_to_select]] = True\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        return X[:, self.support_]\n",
    "\n",
    "    def _mifs(self, X, y, i):\n",
    "        n_samples, n_features = X.shape\n",
    "        x_i = X[:, i]\n",
    "        x_i = x_i.reshape(-1, 1)\n",
    "\n",
    "        scores = np.zeros(n_features)\n",
    "        for j in range(n_features):\n",
    "            if j == i:\n",
    "                continue\n",
    "\n",
    "            x_j = X[:, j]\n",
    "            x_j = x_j.reshape(-1, 1)\n",
    "\n",
    "            x_ij = np.concatenate([x_i, x_j], axis=1)\n",
    "\n",
    "            knn = NearestNeighbors(n_neighbors=self.n_neighbors)\n",
    "            knn.fit(x_ij)\n",
    "            d_ij, _ = knn.kneighbors(x_ij)\n",
    "\n",
    "            d_i = d_ij[:, 0]\n",
    "            d_j = d_ij[:, 1]\n",
    "\n",
    "            mi = self._mi(d_i, d_j)\n",
    "            scores[j] = mi\n",
    "\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def _mi(self, d_i, d_j):\n",
    "        n_samples = len(d_i)\n",
    "        n_bins = int(np.sqrt(n_samples))\n",
    "\n",
    "        hist_2d, _, _ = np.histogram2d(d_i, d_j, bins=n_bins)\n",
    "        p_ij = hist_2d / n_samples\n",
    "\n",
    "        p_i = np.sum(p_ij, axis=1)\n",
    "        p_j = np.sum(p_ij, axis=0)\n",
    "\n",
    "        mi = np.sum(p_ij * np.log(p_ij / np.outer(p_i, p_j) + 1e-10))\n",
    "\n",
    "        return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 features, we properly classified 158/200 clients.\n",
      "Using 12 features, we properly classified 152/200 clients.\n",
      "Using 17 features, we properly classified 133/200 clients.\n",
      "Using 17 features, we properly classified 126/200 clients.\n",
      "Using 14 features, we properly classified 159/200 clients.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5300.0, 5200.0, 3250.0, 2900.0, 5150.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MIFS2(FeatureSelectorWrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(MutualInformationFeatureSelector())\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        support = self.model._support_mask\n",
    "        return X[:, support]\n",
    "    \n",
    "model = RandomForestClassifier(max_depth=5)\n",
    "model = QuadraticDiscriminantAnalysis()\n",
    "feature_selector = MIFS2()\n",
    "k_folds = 5\n",
    "\n",
    "scores = cv(X, y, model, k_folds, feature_selector=feature_selector)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 19 features, we properly classified 103/200 clients.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m feature_selector \u001b[38;5;241m=\u001b[39m MIFS2()\n\u001b[1;32m     17\u001b[0m k_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 19\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_selector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_selector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m scores\n",
      "File \u001b[0;32m~/github/AML-2024L-Offer-Acceptance-Prediction/src/train.py:95\u001b[0m, in \u001b[0;36mcv\u001b[0;34m(X, y, model, k_folds, feature_selector)\u001b[0m\n\u001b[1;32m     92\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m X[test_indices], y[test_indices]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_selector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[43mfeature_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m feature_selector\u001b[38;5;241m.\u001b[39mtransform(X_train)\n\u001b[1;32m     97\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m feature_selector\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mMIFS2.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 357\u001b[0m, in \u001b[0;36mMutualInformationFeatureSelector.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    351\u001b[0m S_mi \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# FIND FIRST FEATURE\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m xy_MI \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mget_first_mi_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# choose the best, add it to S, remove it from F\u001b[39;00m\n\u001b[1;32m    360\u001b[0m S, F \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_remove(S, F, bn\u001b[38;5;241m.\u001b[39mnanargmax(xy_MI))\n",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m, in \u001b[0;36mget_first_mi_vector\u001b[0;34m(MI_FS, k)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03mCalculates the Mututal Information between each feature in X and y.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03mThis function is for when |S| = 0. We select the first feautre in S.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m n, p \u001b[38;5;241m=\u001b[39m MI_FS\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 74\u001b[0m MIs \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMI_FS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_get_first_mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMI_FS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MIs\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "Cell \u001b[0;32mIn[9], line 83\u001b[0m, in \u001b[0;36m_get_first_mi\u001b[0;34m(i, k, MI_FS)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m MI_FS\u001b[38;5;241m.\u001b[39mcategorical:\n\u001b[1;32m     82\u001b[0m     x \u001b[38;5;241m=\u001b[39m MI_FS\u001b[38;5;241m.\u001b[39mX[:, i]\u001b[38;5;241m.\u001b[39mreshape((n, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 83\u001b[0m     MI \u001b[38;5;241m=\u001b[39m \u001b[43m_mi_dc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMI_FS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mvars\u001b[39m \u001b[38;5;241m=\u001b[39m (MI_FS\u001b[38;5;241m.\u001b[39mX[:, i]\u001b[38;5;241m.\u001b[39mreshape((n, \u001b[38;5;241m1\u001b[39m)), MI_FS\u001b[38;5;241m.\u001b[39my)\n",
      "Cell \u001b[0;32mIn[9], line 118\u001b[0m, in \u001b[0;36m_mi_dc\u001b[0;34m(x, y, k)\u001b[0m\n\u001b[1;32m    116\u001b[0m Nx \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m yi \u001b[38;5;129;01min\u001b[39;00m y:\n\u001b[0;32m--> 118\u001b[0m     Nx\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# find the distance of the kth in-class point\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m classes:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MIFS2(BaseFeatureSelector):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = MutualInformationFeatureSelector()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        support = self.model._support_mask\n",
    "        return X[:, support]\n",
    "    \n",
    "\n",
    "# model = RandomForestClassifier(max_depth=5)\n",
    "model = LogisticRegression()\n",
    "feature_selector = MIFS2()\n",
    "k_folds = 5\n",
    "\n",
    "scores = cv(X, y, model, k_folds, feature_selector=feature_selector)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2016, Daniel Homola\n",
    "# All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "# * Redistributions of source code must retain the above copyright notice, this\n",
    "#   list of conditions and the following disclaimer.\n",
    "\n",
    "# * Redistributions in binary form must reproduce the above copyright notice,\n",
    "#   this list of conditions and the following disclaimer in the documentation\n",
    "#   and/or other materials provided with the distribution.\n",
    "\n",
    "# * Neither the name of mifs nor the names of its\n",
    "#   contributors may be used to endorse or promote products derived from\n",
    "#   this software without specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "NUM_CORES = cpu_count()\n",
    "\n",
    "def get_mi_vector(MI_FS, F, s):\n",
    "    \"\"\"\n",
    "    Calculates the Mututal Information between each feature in F and s.\n",
    "\n",
    "    This function is for when |S| > 1. s is the previously selected feature.\n",
    "    We exploite the fact that this step is embarrassingly parallel.\n",
    "    \"\"\"\n",
    "\n",
    "    MIs = Parallel(n_jobs=MI_FS.n_jobs)(delayed(_get_mi)(f, s, MI_FS)\n",
    "                                        for f in F)\n",
    "    return MIs\n",
    "\n",
    "\n",
    "def _get_mi(f, s, MI_FS):\n",
    "    \n",
    "    n, p = MI_FS.X.shape\n",
    "    if MI_FS.method in ['JMI', 'JMIM']:\n",
    "        # JMI & JMIM\n",
    "        joint = MI_FS.X[:, (s, f)]\n",
    "        if MI_FS.categorical:\n",
    "            MI = _mi_dc(joint, MI_FS.y, MI_FS.k)\n",
    "        else:\n",
    "            vars = (joint, MI_FS.y)\n",
    "            MI = _mi_cc(vars, MI_FS.k)\n",
    "    else:\n",
    "        # MRMR\n",
    "        vars = (MI_FS.X[:, s].reshape(n, 1), MI_FS.X[:, f].reshape(n, 1))\n",
    "        MI = _mi_cc(vars, MI_FS.k)\n",
    "\n",
    "    # MI must be non-negative\n",
    "    if MI > 0:\n",
    "        return MI\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_first_mi_vector(MI_FS, k):\n",
    "    \"\"\"\n",
    "    Calculates the Mututal Information between each feature in X and y.\n",
    "\n",
    "    This function is for when |S| = 0. We select the first feautre in S.\n",
    "    \"\"\"\n",
    "    n, p = MI_FS.X.shape\n",
    "    MIs = Parallel(n_jobs=MI_FS.n_jobs)(delayed(_get_first_mi)(i, k, MI_FS)\n",
    "                                        for i in range(p))\n",
    "    return MIs\n",
    "\n",
    "\n",
    "def _get_first_mi(i, k, MI_FS):\n",
    "    n, p = MI_FS.X.shape\n",
    "    if MI_FS.categorical:\n",
    "        x = MI_FS.X[:, i].reshape((n, 1))\n",
    "        MI = _mi_dc(x, MI_FS.y, k)\n",
    "    else:\n",
    "        vars = (MI_FS.X[:, i].reshape((n, 1)), MI_FS.y)\n",
    "        MI = _mi_cc(vars, k)\n",
    "\n",
    "    # MI must be non-negative\n",
    "    if MI > 0:\n",
    "        return MI\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _mi_dc(x, y, k):\n",
    "    \"\"\"\n",
    "    Calculates the mututal information between a continuous vector x and a\n",
    "    disrete class vector y.\n",
    "\n",
    "    This implementation can calculate the MI between the joint distribution of\n",
    "    one or more continuous variables (X[:, 1:3]) with a discrete variable (y).\n",
    "\n",
    "    Thanks to Adam Pocock, the author of the FEAST package for the idea.\n",
    "\n",
    "    Brian C. Ross, 2014, PLOS ONE\n",
    "    Mutual Information between Discrete and Continuous Data Sets\n",
    "    \"\"\"\n",
    "\n",
    "    y = y.flatten()\n",
    "    n = x.shape[0]\n",
    "    classes = np.unique(y)\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    # distance to kth in-class neighbour\n",
    "    d2k = np.empty(n)\n",
    "    # number of points within each point's class\n",
    "    Nx = []\n",
    "    for yi in y:\n",
    "        Nx.append(np.sum(y == yi))\n",
    "\n",
    "    # find the distance of the kth in-class point\n",
    "    for c in classes:\n",
    "        mask = np.where(y == c)[0]\n",
    "        knn.fit(x[mask, :])\n",
    "        d2k[mask] = knn.kneighbors()[0][:, -1]\n",
    "\n",
    "    # find the number of points within the distance of the kth in-class point\n",
    "    knn.fit(x)\n",
    "    m = knn.radius_neighbors(radius=d2k, return_distance=False)\n",
    "    m = [i.shape[0] for i in m]\n",
    "\n",
    "    # calculate MI based on Equation 2 in Ross 2014\n",
    "    MI = psi(n) - np.mean(psi(Nx)) + psi(k) - np.mean(psi(m))\n",
    "    return MI\n",
    "\n",
    "\n",
    "def _mi_cc(variables, k=1):\n",
    "    \"\"\"\n",
    "    Returns the mutual information between any number of variables.\n",
    "\n",
    "    Here it is used to estimate MI between continuous X(s) and y.\n",
    "    Written by Gael Varoquaux:\n",
    "    https://gist.github.com/GaelVaroquaux/ead9898bd3c973c40429\n",
    "    \"\"\"\n",
    "\n",
    "    all_vars = np.hstack(variables)\n",
    "    return (sum([_entropy(X, k=k) for X in variables]) -\n",
    "            _entropy(all_vars, k=k))\n",
    "\n",
    "\n",
    "def _nearest_distances(X, k=1):\n",
    "    \"\"\"\n",
    "    Returns the distance to the kth nearest neighbor for every point in X\n",
    "    \"\"\"\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric='chebyshev')\n",
    "    knn.fit(X)\n",
    "    # the first nearest neighbor is itself\n",
    "    d, _ = knn.kneighbors(X)\n",
    "    # returns the distance to the kth nearest neighbor\n",
    "    return d[:, -1]\n",
    "\n",
    "\n",
    "def _entropy(X, k=1):\n",
    "    \"\"\"\n",
    "    Returns the entropy of the X.\n",
    "\n",
    "    Written by Gael Varoquaux:\n",
    "    https://gist.github.com/GaelVaroquaux/ead9898bd3c973c40429\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The data the entropy of which is computed\n",
    "    k : int, optional\n",
    "        number of nearest neighbors for density estimation\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Kozachenko, L. F. & Leonenko, N. N. 1987 Sample estimate of entropy\n",
    "    of a random vector. Probl. Inf. Transm. 23, 95-101.\n",
    "    See also: Evans, D. 2008 A computationally efficient estimator for\n",
    "    mutual information, Proc. R. Soc. A 464 (2093), 1203-1215.\n",
    "    and:\n",
    "\n",
    "    Kraskov A, Stogbauer H, Grassberger P. (2004). Estimating mutual\n",
    "    information. Phys Rev E 69(6 Pt 2):066138.\n",
    "\n",
    "    F. Perez-Cruz, (2008). Estimation of Information Theoretic Measures\n",
    "    for Continuous Random Variables. Advances in Neural Information\n",
    "    Processing Systems 21 (NIPS). Vancouver (Canada), December.\n",
    "    return d*mean(log(r))+log(volume_unit_ball)+log(n-1)-log(k)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Distance to kth nearest neighbor\n",
    "    r = _nearest_distances(X, k)\n",
    "    n, d = X.shape\n",
    "    volume_unit_ball = (np.pi ** (.5 * d)) / gamma(.5 * d + 1)\n",
    "    return (d * np.mean(np.log(r + np.finfo(X.dtype).eps)) +\n",
    "            np.log(volume_unit_ball) + psi(n) - psi(k))\n",
    "\n",
    "class MutualInformationFeatureSelector(BaseEstimator, SelectorMixin):\n",
    "    \"\"\"\n",
    "    MI_FS stands for Mutual Information based Feature Selection.\n",
    "    This class contains routines for selecting features using both\n",
    "    continuous and discrete y variables. Three selection algorithms are\n",
    "    implemented: JMI, JMIM and MRMR.\n",
    "\n",
    "    This implementation tries to mimic the scikit-learn interface, so use fit,\n",
    "    transform or fit_transform, to run the feature selection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    method : string, default = 'JMI'\n",
    "        Which mutual information based feature selection method to use:\n",
    "        - 'JMI' : Joint Mutual Information [1]\n",
    "        - 'JMIM' : Joint Mutual Information Maximisation [2]\n",
    "        - 'MRMR' : Max-Relevance Min-Redundancy [3]\n",
    "\n",
    "    k : int, default = 5\n",
    "        Sets the number of samples to use for the kernel density estimation\n",
    "        with the kNN method. Kraskov et al. recommend a small integer between\n",
    "        3 and 10.\n",
    "\n",
    "    n_features : int or string, default = 'auto'\n",
    "        If int, it sets the number of features that has to be selected from X.\n",
    "        If 'auto' this is determined automatically based on the amount of\n",
    "        mutual information the previously selected features share with y.\n",
    "\n",
    "    categorical : Boolean, default = True\n",
    "        If True, y is assumed to be a categorical class label. If False, y is\n",
    "        treated as a continuous. Consequently this parameter determines the\n",
    "        method of estimation of the MI between the predictors in X and y.\n",
    "\n",
    "    n_jobs : int, optional (default=1)\n",
    "        The number of threads to open if possible.\n",
    "\n",
    "    verbose : int, default=0\n",
    "        Controls verbosity of output:\n",
    "        - 0: no output\n",
    "        - 1: displays selected features\n",
    "        - 2: displays selected features and mutual information\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    n_features_ : int\n",
    "        The number of selected features.\n",
    "\n",
    "    support_ : array of length X.shape[1]\n",
    "        The mask array of selected features.\n",
    "\n",
    "    ranking_ : array of shape n_features\n",
    "        The feature ranking of the selected features, with the first being\n",
    "        the first feature selected with largest marginal MI with y, followed by\n",
    "        the others with decreasing MI.\n",
    "\n",
    "    mi_ : array of shape n_features\n",
    "        The JMIM of the selected features. Usually this a monotone decreasing\n",
    "        array of numbers converging to 0. One can use this to estimate the\n",
    "        number of features to select. In fact this is what n_features='auto''\n",
    "        tries to do heuristically.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    import pandas as pd\n",
    "    import mifs\n",
    "\n",
    "    # load X and y\n",
    "    X = pd.read_csv('my_X_table.csv', index_col=0).values\n",
    "    y = pd.read_csv('my_y_vector.csv', index_col=0).values\n",
    "\n",
    "    # define MI_FS feature selection method\n",
    "    feat_selector = mifs.MutualInformationFeatureSelector()\n",
    "\n",
    "    # find all relevant features\n",
    "    feat_selector.fit(X, y)\n",
    "\n",
    "    # check selected features\n",
    "    feat_selector.support_\n",
    "\n",
    "    # check ranking of features\n",
    "    feat_selector.ranking_\n",
    "\n",
    "    # call transform() on X to filter it down to selected features\n",
    "    X_filtered = feat_selector.transform(X)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    [1] H. Yang and J. Moody, \"Data Visualization and Feature Selection: New\n",
    "        Algorithms for Nongaussian Data\"\n",
    "        NIPS 1999\n",
    "    [2] Bennasar M., Hicks Y., Setchi R., \"Feature selection using Joint Mutual\n",
    "        Information Maximisation\"\n",
    "        Expert Systems with Applications, Vol. 42, Issue 22, Dec 2015\n",
    "    [3] H. Peng, Fulmi Long, C. Ding, \"Feature selection based on mutual\n",
    "        information criteria of max-dependency, max-relevance,\n",
    "        and min-redundancy\"\n",
    "        Pattern Analysis & Machine Intelligence 2005\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='JMI', k=5, n_features='auto', categorical=True,\n",
    "                 n_jobs=1, verbose=0):\n",
    "        self.method = method\n",
    "        self.k = k\n",
    "        self.n_features = n_features\n",
    "        self.categorical = categorical\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        self._support_mask = None\n",
    "\n",
    "    def _get_support_mask(self):\n",
    "        if self._support_mask is None:\n",
    "            raise ValueError('mRMR has not been fitted yet!')\n",
    "        return self._support_mask\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the MI_FS feature selection with the chosen MI_FS method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if n_jobs is negative\n",
    "        if self.n_jobs < 0:\n",
    "            self.n_jobs = NUM_CORES - self.n_jobs\n",
    "\n",
    "        self.X, y = self._check_params(X, y)\n",
    "        n, p = X.shape\n",
    "        self.y = y.reshape((n, 1))\n",
    "\n",
    "        # list of selected features\n",
    "        S = []\n",
    "        # list of all features\n",
    "        F = list(range(p))\n",
    "\n",
    "        if self.n_features != 'auto':\n",
    "            feature_mi_matrix = np.zeros((self.n_features, p))\n",
    "        else:\n",
    "            feature_mi_matrix = np.zeros((n, p))\n",
    "        feature_mi_matrix[:] = np.nan\n",
    "        S_mi = []\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # FIND FIRST FEATURE\n",
    "        # ---------------------------------------------------------------------\n",
    "\n",
    "        xy_MI = np.array(get_first_mi_vector(self, self.k))\n",
    "\n",
    "        # choose the best, add it to S, remove it from F\n",
    "        S, F = self._add_remove(S, F, bn.nanargmax(xy_MI))\n",
    "        S_mi.append(bn.nanmax(xy_MI))\n",
    "\n",
    "        # notify user\n",
    "        if self.verbose > 0:\n",
    "            self._print_results(S, S_mi)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # FIND SUBSEQUENT FEATURES\n",
    "        # ---------------------------------------------------------------------\n",
    "        if self.n_features == 'auto': n_features = np.inf\n",
    "        else: n_features = self.n_features\n",
    "         \n",
    "        while len(S) < n_features:\n",
    "            # loop through the remaining unselected features and calculate MI\n",
    "            s = len(S) - 1\n",
    "            feature_mi_matrix[s, F] = get_mi_vector(self, F, S[-1])\n",
    "\n",
    "            # make decision based on the chosen FS algorithm\n",
    "            fmm = feature_mi_matrix[:len(S), F]\n",
    "            if self.method == 'JMI':\n",
    "                selected = F[bn.nanargmax(bn.nansum(fmm, axis=0))]\n",
    "            elif self.method == 'JMIM':\n",
    "                if bn.allnan(bn.nanmin(fmm, axis = 0)):\n",
    "                    break\n",
    "                selected = F[bn.nanargmax(bn.nanmin(fmm, axis=0))]\n",
    "            elif self.method == 'MRMR':\n",
    "                if bn.allnan(bn.nanmean(fmm, axis = 0)):\n",
    "                    break\n",
    "                MRMR = xy_MI[F] - bn.nanmean(fmm, axis=0)\n",
    "                selected = F[bn.nanargmax(MRMR)]\n",
    "                S_mi.append(bn.nanmax(MRMR))\n",
    "\n",
    "            # record the JMIM of the newly selected feature and add it to S\n",
    "            if self.method != 'MRMR':\n",
    "                S_mi.append(bn.nanmax(bn.nanmin(fmm, axis=0)))\n",
    "            S, F = self._add_remove(S, F, selected)\n",
    "\n",
    "            # notify user\n",
    "            if self.verbose > 0:\n",
    "                self._print_results(S, S_mi)\n",
    "\n",
    "            # if n_features == 'auto', let's check the S_mi to stop\n",
    "            if self.n_features == 'auto' and len(S) > 10:\n",
    "                # smooth the 1st derivative of the MI values of previously sel\n",
    "                MI_dd = signal.savgol_filter(S_mi[1:], 9, 2, 1)\n",
    "                # does the mean of the last 5 converge to 0?\n",
    "                if np.abs(np.mean(MI_dd[-5:])) < 1e-3:\n",
    "                    break\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # SAVE RESULTS\n",
    "        # ---------------------------------------------------------------------\n",
    "\n",
    "        self.n_features_ = len(S)\n",
    "        self._support_mask = np.zeros(p, dtype=bool)\n",
    "        self._support_mask[S] = True\n",
    "        self.ranking_ = S\n",
    "        self.mi_ = S_mi\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _isinteger(self, x):\n",
    "        return np.all(np.equal(np.mod(x, 1), 0))\n",
    "\n",
    "    def _check_params(self, X, y):\n",
    "        # checking input data and scaling it if y is continuous\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        if not self.categorical:\n",
    "            ss = StandardScaler()\n",
    "            X = ss.fit_transform(X)\n",
    "            y = ss.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "        # sanity checks\n",
    "        methods = ['JMI', 'JMIM', 'MRMR']\n",
    "        if self.method not in methods:\n",
    "            raise ValueError('Please choose one of the following methods:\\n' +\n",
    "                             '\\n'.join(methods))\n",
    "\n",
    "        if not isinstance(self.k, int):\n",
    "            raise ValueError(\"k must be an integer.\")\n",
    "        if self.k < 1:\n",
    "            raise ValueError('k must be larger than 0.')\n",
    "        if self.categorical and np.any(self.k > np.bincount(y)):\n",
    "            raise ValueError('k must be smaller than your smallest class.')\n",
    "\n",
    "        if not isinstance(self.categorical, bool):\n",
    "            raise ValueError('Categorical must be Boolean.')\n",
    "        \n",
    "        if not self.categorical and self._isinteger(y):\n",
    "            print ('Are you sure y is continuous? It seems to be discrete.')\n",
    "        if self._isinteger(X):\n",
    "            print ('The values of X seem to be discrete. MI_FS will treat them'\n",
    "                   'as continuous.')\n",
    "        return X, y\n",
    "\n",
    "    def _add_remove(self, S, F, i):\n",
    "        \"\"\"\n",
    "        Helper function: removes ith element from F and adds it to S.\n",
    "        \"\"\"\n",
    "\n",
    "        S.append(i)\n",
    "        F.remove(i)\n",
    "        return S, F\n",
    "\n",
    "    def _print_results(self, S, MIs):\n",
    "        out = ''\n",
    "        if self.n_features == 'auto':\n",
    "            out += 'Auto selected feature #' + str(len(S)) + ' : ' + str(S[-1])\n",
    "        else:\n",
    "            out += ('Selected feature #' + str(len(S)) + ' / ' +\n",
    "                    str(self.n_features) + ' : ' + str(S[-1]))\n",
    "\n",
    "        if self.verbose > 1:\n",
    "            out += ', ' + self.method + ' : ' + str(MIs[-1])\n",
    "        print (out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
